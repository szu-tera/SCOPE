[project]
name = "scope"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10.0"
dependencies = [
    "accelerate>=1.10.1",
    "codetiming>=1.4.0",
    "datasets>=4.1.1",
    "debugpy>=1.8.17",
    "dill>=0.4.0",
    "fastapi[standard]>=0.115.0",
    "flash-attn",
    "flashinfer-python",
    "grpcio>=1.62.1",
    "hf-transfer>=0.1.9",
    "hydra-core>=1.3.2",
    "latex2sympy2-extended>=1.10.2",
    "liger-kernel>=0.6.2",
    "math-verify>=0.8.0",
    "mathruler>=0.1.0",
    "numpy<2.0.0",
    "nvidia-ml-py>=12.560.30",
    "optree>=0.13.0",
    "pandas>=2.3.3",
    "peft>=0.17.1",
    "pre-commit>=4.3.0",
    "py-spy>=0.4.1",
    "pyarrow>=15.0.0",
    "pybind11>=3.0.1",
    "pydantic>=2.9",
    "pyext>=0.7",
    "pylatexenc>=2.10",
    "pytest>=8.4.2",
    "qwen-vl-utils>=0.0.14",
    "ray[default]>=2.47.1",
    "ruff>=0.13.2",
    "swanlab>=0.6.10",
    "tensorboard>=2.20.0",
    "tensordict==0.6.2",
    "torch==2.6.0",
    "torchaudio==2.6.0",
    "torchdata>=0.11.0",
    "torchvision==0.21.0",
    "transformers[hf-xet]>=4.51.0",
    "vllm==0.8.5.post1",
    "wandb>=0.22.1",
]

[tool.uv.sources]
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl" }
flashinfer-python = { url = "https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2.post1/flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl" }
